# Ollama Configuration
OLLAMA_SCHEMA=http
OLLAMA_HOST=host.docker.internal
OLLAMA_PORT=11434
OLLAMA_TEMPERATURE=0.5

# Weaviate Configuration
WEAVIATE_HOST=weaviate
WEAVIATE_PORT=8080
WEAVIATE_ALPHA=0.5

# AI Model Configuration
MODEL_NAME=llama3.2:3b
LOCAL_MODEL_NAME=all-MiniLM-L6-v2
