version: '3.8'

services:
  chatbot:
    build: .
    container_name: ttyd_chatbot
    restart: always
    ports:
      - "5000:5000"
    volumes:
      - ./instructions.txt:/app/instructions.txt
      - ./my_files:/app/my_files
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      ollama_check_server:
        condition: service_completed_successfully  # ‚úÖ Only start if Ollama check succeeds

  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: weaviate
    restart: always
    ports:
      - "8080:8080"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
    depends_on:
      ollama_check_server:
        condition: service_completed_successfully  # ‚úÖ Only start if Ollama check succeeds

  ollama:
    image: ollama/ollama:latest  # ‚úÖ Official Ollama container
    container_name: ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama  # ‚úÖ Persistent model storage
    environment:
      - OLLAMA_MODEL=${MODEL_NAME}  # ‚úÖ Auto-load the model at startup
    extra_hosts:
      - "host.docker.internal:host-gateway"

  ollama_check_server:
    image: curlimages/curl:latest
    container_name: ollama_check_server
    restart: "no"
    env_file:
      - .env  # ‚úÖ Load Ollama settings from .env
    depends_on:
      ollama:
        condition: service_started  # ‚úÖ Ensure Ollama starts first
    command: >
      sh -c "
      echo 'üîç Checking Ollama server at ${OLLAMA_SCHEMA}://${OLLAMA_HOST}:${OLLAMA_PORT}...';
      until curl -s --head --request GET '${OLLAMA_SCHEMA}://${OLLAMA_HOST}:${OLLAMA_PORT}' | grep '200 OK'; do
        echo '‚è≥ Waiting for Ollama server...';
        sleep 2;
      done;
      echo '‚úÖ Ollama server is up and running!';
      exit 0;
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -s --head --request GET ${OLLAMA_SCHEMA}://${OLLAMA_HOST}:${OLLAMA_PORT} | grep '200 OK'"]



volumes:
  ollama_data:  # ‚úÖ Persistent volume for storing models
