networks:
  ttyd_internal_network:  # ‚úÖ Define a private Docker network
    driver: bridge

services:
  chatbot:
    build: .
    container_name: ttyd_chatbot
    restart: always
    privileged: true
    ports:
      - "${TTYD_API_PORT}:${TTYD_API_PORT}"
    volumes:
      - ./instructions.txt:/app/instructions.txt
      - ./my_files:/app/my_files
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ttyd_internal_network
    depends_on:
      ollama_check_server:
        condition: service_completed_successfully  # ‚úÖ Only start if model check succeeds

  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: ttyd_weaviate
    restart: always
    privileged: true
    expose:
      - "8080"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
    networks:
      - ttyd_internal_network
    depends_on:
      ollama_check_server:
        condition: service_completed_successfully  # ‚úÖ Only start if model check succeeds

  ollama:
    image: ollama/ollama:latest  # ‚úÖ Ollama Server
    container_name: ttyd_ollama
    restart: always
    privileged: true
    volumes:
      - ollama_data:/root/.ollama  # ‚úÖ Persistent model storage
    expose:
      - "11434"  # ‚úÖ Only accessible inside the Docker network

    networks:
      - ttyd_internal_network

  ollama_model_runner:
    image: curlimages/curl:latest
    container_name: ttyd_ollama_model_runner
    restart: "no"
    privileged: true
    env_file:
      - .env
    networks:
      - ttyd_internal_network
    depends_on:
      ollama:
        condition: service_started  # ‚úÖ Ensure Ollama starts first
    command: >
      sh -c "
      echo 'üîç Waiting for Ollama server to be ready...';
      until curl -s --head --request GET http://ollama:11434 | grep '200 OK'; do
        echo '‚è≥ Waiting for Ollama server...';
        sleep 2;
      done;
      echo '‚úÖ Ollama server is up and running!';

      echo '‚¨áÔ∏è Pulling model \"${MODEL_NAME}\"...';
      curl --location 'http://ollama:11434/api/pull' \
      --header 'Content-Type: application/json' \
      --data '{\"model\": \"'${MODEL_NAME}'\"}';
      echo '‚úÖ Model \"${MODEL_NAME}\" is pulled!';

      echo 'üöÄ Loading model \"${MODEL_NAME}\" with keep_alive: 24h...';
      curl --location 'http://ollama:11434/api/generate' \
      --header 'Content-Type: application/json' \
      --data '{\"model\": \"'${MODEL_NAME}'\", \"keep_alive\": \"24h\"}';
      echo '‚úÖ Model \"${MODEL_NAME}\" is now preloaded!';
      exit 0;
      "

  ollama_check_server:
    image: curlimages/curl:latest
    container_name: ttyd_ollama_check_server
    restart: "no"
    privileged: true
    env_file:
      - .env
    depends_on:
      ollama_model_runner:
        condition: service_completed_successfully
    networks:
      - ttyd_internal_network
    command: >
      sh -c "
      echo 'üîç Checking if model \"${MODEL_NAME}\" is loaded via /api/ps...';
      until curl -s 'http://ollama:11434/api/ps' | grep -E -q '\"model\"\s*:\s*\"'${MODEL_NAME}'\"'; do
        echo '‚è≥ Waiting for model \"${MODEL_NAME}\" to load...';
        sleep 2;
      done;
      echo '‚úÖ Model \"${MODEL_NAME}\" is loaded!';
      exit 0;
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -s --head --request GET http://ollama:11434 | grep '200 OK'"]

volumes:
  ollama_data:  # ‚úÖ Persistent volume for storing models
